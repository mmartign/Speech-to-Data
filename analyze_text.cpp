// -*- coding: utf-8 -*-
//
// This file is part of the Spazio IT Speech-to-Data project.
//
// Copyright (C) 2025 Spazio IT
// Spazio - IT Soluzioni Informatiche s.a.s.
// via Manzoni 40
// 46051 San Giorgio Bigarello
// https://spazioit.com
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 2 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program. If not, see https://www.gnu.org/licenses/.
//
#include <fstream>
#include <cstdlib>
#include <iostream>
#include <sstream>
#include <string>
#include <map>
#include <thread>
#include <mutex>
#include <atomic>
#include <vector>
#include <algorithm>
#include <utility>
#include <openai.hpp>

using json = nlohmann::json;

// Global config
std::string OPENWEBUI_URL;
std::string API_KEY;
std::string MODEL_NAME;
std::string KNOWLEDGE_BASE_IDS;
std::string PROMPT;
std::string TEMP_PROMPT;
std::string TRIGGER_START;
std::string TRIGGER_STOP;
std::string TRIGGER_TEMP_CHECK;

std::mutex analysis_mutex;
std::atomic<int> counter_value{0};
std::atomic<int> active_analyses{0};
std::mutex tts_mutex;
std::once_flag openai_init_flag;

class AnalysisSession {
public:
    AnalysisSession(std::mutex& mutex, std::atomic<int>& active_count)
        : lock_(mutex), active_count_(active_count) {
        ++active_count_;
    }

    ~AnalysisSession() {
        --active_count_;
    }

private:
    std::unique_lock<std::mutex> lock_;
    std::atomic<int>& active_count_;
};

std::string strip_trailing_newlines(std::string text) {
    while (!text.empty() && (text.back() == '\n' || text.back() == '\r')) {
        text.pop_back();
    }
    return text;
}

std::string escape_for_quotes(const std::string& text) {
    std::string escaped;
    escaped.reserve(text.size());
    for (char c : text) {
        if (c == '\\' || c == '\"') {
            escaped.push_back('\\');
        }
        escaped.push_back(c);
    }
    return escaped;
}

void speak_text(const std::string& text) {
    std::string trimmed = strip_trailing_newlines(text);
    if (trimmed.empty()) {
        return;
    }

    trimmed = "Announciator: " + trimmed;

    const std::string escaped = escape_for_quotes(trimmed);
    const std::string cmd = "say \"" + escaped + "\" >/dev/null 2>&1 &";

    std::lock_guard<std::mutex> lock(tts_mutex);
    std::system(cmd.c_str());
}

void say_info(const std::string& message) {
    std::cout << message;
    speak_text(message);
}

void say_error(const std::string& message) {
    std::cerr << message;
    speak_text(message);
}

// Simple INI parser
std::map<std::string, std::string> parse_ini(const std::string& filename) {
    std::ifstream file(filename);
    std::map<std::string, std::string> config;
    std::string line, section;

    while (std::getline(file, line)) {
        // Remove comments
        size_t comment_pos = line.find_first_of(";#");
        if (comment_pos != std::string::npos) line = line.substr(0, comment_pos);

        // Trim whitespace
        line.erase(0, line.find_first_not_of(" \t\r\n"));
        line.erase(line.find_last_not_of(" \t\r\n") + 1);

        if (line.empty()) continue;

        if (line.front() == '[' && line.back() == ']') {
            section = line.substr(1, line.size() - 2);
        } else {
            size_t eq_pos = line.find('=');
            if (eq_pos != std::string::npos) {
                std::string key = line.substr(0, eq_pos);
                std::string value = line.substr(eq_pos + 1);
                key.erase(0, key.find_first_not_of(" \t\r\n"));
                key.erase(key.find_last_not_of(" \t\r\n") + 1);
                value.erase(0, value.find_first_not_of(" \t\r\n"));
                value.erase(value.find_last_not_of(" \t\r\n") + 1);
                config[section + "." + key] = value;
            }
        }
    }

    return config;
}

// Load config
bool load_config(const std::string& path) {
    std::ifstream file_check(path);
    if (!file_check.is_open()) {
        say_error("Unable to open config file: " + path + "\n");
        return false;
    }
    file_check.close();

    auto config = parse_ini(path);

    std::vector<std::string> missing_keys;
    auto require_value = [&](const std::string& key, std::string& destination) {
        auto it = config.find(key);
        if (it == config.end() || it->second.empty()) {
            missing_keys.push_back(key);
            return;
        }
        destination = it->second;
    };

    require_value("openai.base_url", OPENWEBUI_URL);
    require_value("openai.api_key", API_KEY);
    require_value("openai.model_name", MODEL_NAME);
    require_value("prompts.prompt", PROMPT);
    require_value("prompts.temp_prompt", TEMP_PROMPT);
    require_value("triggers.start", TRIGGER_START);
    require_value("triggers.stop", TRIGGER_STOP);
    require_value("triggers.temp_check", TRIGGER_TEMP_CHECK);

    auto kb_it = config.find("analysis.knowledge_base_ids");
    KNOWLEDGE_BASE_IDS = (kb_it != config.end()) ? kb_it->second : std::string{};

    if (!missing_keys.empty()) {
        std::ostringstream oss;
        oss << "Missing required config values:";
        for (const auto& key : missing_keys) {
            oss << ' ' << key;
        }
        oss << "\n";
        say_error(oss.str());
        return false;
    }

    std::transform(TRIGGER_START.begin(), TRIGGER_START.end(), TRIGGER_START.begin(), ::tolower);
    std::transform(TRIGGER_STOP.begin(), TRIGGER_STOP.end(), TRIGGER_STOP.begin(), ::tolower);
    std::transform(TRIGGER_TEMP_CHECK.begin(), TRIGGER_TEMP_CHECK.end(), TRIGGER_TEMP_CHECK.begin(), ::tolower);

    if (KNOWLEDGE_BASE_IDS.empty()) {
        say_error("Warning: analysis.knowledge_base_ids is not set; knowledge base lookups will be skipped.\n");
    }

    return true;
}

// Substring check
bool contains_substring(const std::string& str, const std::string& sub) {
    return sub.empty() || str.find(sub) != std::string::npos;
}

// Safely extract a textual message content from an OpenAI-style response
std::string extract_message_content(const json& response) {
    const auto choices_it = response.find("choices");
    if (choices_it == response.end() || !choices_it->is_array() || choices_it->empty()) {
        return {};
    }

    const auto& first_choice = (*choices_it)[0];
    if (!first_choice.is_object()) {
        return {};
    }

    const auto message_it = first_choice.find("message");
    if (message_it == first_choice.end() || !message_it->is_object()) {
        return {};
    }

    const auto content_it = message_it->find("content");
    if (content_it == message_it->end()) {
        return {};
    }

    if (content_it->is_string()) {
        return content_it->get<std::string>();
    }

    if (content_it->is_array()) {
        std::string combined;
        for (const auto& part : *content_it) {
            if (!combined.empty()) {
                combined.push_back('\n');
            }
            combined += part.is_string() ? part.get<std::string>() : part.dump();
        }
        return combined;
    }

    return {};
}

// AI analysis with fresh context for each request
void analyze_text(const std::string& text) {
    AnalysisSession session(analysis_mutex, active_analyses);
    const int analysis_id = ++counter_value;
    say_info("Analysis of Recording[" + std::to_string(analysis_id) + "] Started ------------------->>>\n");

    const std::string filename = "results_analysis" + std::to_string(analysis_id) + ".txt";
    std::ofstream file(filename);
    if (!file.is_open()) {
        say_error("[ERROR] Unable to open results file: " + filename + "\n");
        say_info("Analysis of Recording[" + std::to_string(analysis_id) + "] Finished ------------------->>>\n");
        return;
    }

    file << "Using model: " << MODEL_NAME << "\n";
    file << "Endpoint: " << OPENWEBUI_URL << "\n";
    file << "Prompt: " << PROMPT << "\n" << text << "\n";

    if (!file) {
        say_error("[ERROR] Failed to write analysis header to " + filename + "\n");
    }

    std::string response_string;

    try {
        openai::start({
            API_KEY
        });

        json body = {
            {"model", MODEL_NAME},
            {"messages", {
                {{"role", "system"}, {"content", "You are a helpful assistant."}},
                {{"role", "user"}, {"content", PROMPT + "\n" + text}}
            }},
            {"stream", false},
            {"enable_websearch", true}
        };

        if (!KNOWLEDGE_BASE_IDS.empty()) {
            body["knowledge_base_ids"] = json::array({KNOWLEDGE_BASE_IDS});
        }

        auto chat = openai::chat().create(body);
        response_string = extract_message_content(chat);
        if (response_string.empty()) {
            file << "\n[WARN] No textual content found in primary response. Full payload:\n"
                 << chat.dump(2) << "\n";
            say_error(std::string{"[WARN] Analysis["} + std::to_string(analysis_id) +
                      "] returned no text content; see results file.\n");
        }

        file << "\n\nFull response received:\n" << response_string << "\n";
    } catch (const std::exception& e) {
        file << "\n[ERROR] Analysis[" << analysis_id << "] failed: " << e.what() << "\n";
        say_error(std::string{"[ERROR] Analysis["} + std::to_string(analysis_id) + "] failed: " + e.what() + "\n");
    }

    if (!response_string.empty()) {
        try {
            json summary_body = {
                {"model", MODEL_NAME},
                {"messages", {
                    {{"role", "system"}, {"content", "You are a helpful assistant."}},
                    {{"role", "user"}, {"content", "Provide a concise summary of the following text, Keep it short and informative.\n" + response_string + "\n\n"}}
                }},
                {"stream", false},
                {"enable_websearch", false}
            };

            auto summary_chat = openai::chat().create(summary_body);
            const std::string summary_string = extract_message_content(summary_chat);
            if (summary_string.empty()) {
                file << "\n[WARN] No textual summary returned. Full payload:\n"
                     << summary_chat.dump(2) << "\n";
                say_error(std::string{"[WARN] Summary generation returned no text for Analysis["} +
                          std::to_string(analysis_id) + "]; see results file.\n");
            }

            file << "\nShort summary of response:\n" << summary_string << "\n";
            speak_text("Analysis[" + std::to_string(analysis_id) + "] completed. Summary: " + summary_string);
        } catch (const std::exception& e) {
            file << "\n[ERROR] Summary generation failed: " << e.what() << "\n";
            say_error(std::string{"[ERROR] Summary generation failed for Analysis["} + std::to_string(analysis_id) + "]: " + e.what() + "\n");
        }
    }

    if (!file) {
        say_error("[ERROR] Writing to results file failed for Analysis[" + std::to_string(analysis_id) + "]\n");
    }

    say_info("Analysis of Recording[" + std::to_string(analysis_id) + "] Finished ------------------->>>\n");
}

void temp_analyze_text(const std::string& text) {
    AnalysisSession session(analysis_mutex, active_analyses);
    const int analysis_id = ++counter_value;
    say_info("Temporary_Analysis of Recording[" + std::to_string(analysis_id) + "] Started ------------------->>>\n");

    const std::string filename = "tmp_results_analysis" + std::to_string(analysis_id) + ".txt";
    std::ofstream file(filename);
    if (!file.is_open()) {
        say_error("[ERROR] Unable to open results file: " + filename + "\n");
        say_info("Temporary Analysis of Recording[" + std::to_string(analysis_id) + "] Finished ------------------->>>\n");
        return;
    }

    file << "Using model: " << MODEL_NAME << "\n";
    file << "Endpoint: " << OPENWEBUI_URL << "\n";
    file << "Prompt: " << TEMP_PROMPT << "\n" << text << "\n";

    if (!file) {
        say_error("[ERROR] Failed to write analysis header to " + filename + "\n");
    }

    std::string response_string;

    try {
        openai::start({
            API_KEY
        });

        json body = {
            {"model", MODEL_NAME},
            {"messages", {
                {{"role", "system"}, {"content", "You are a helpful assistant."}},
                {{"role", "user"}, {"content", TEMP_PROMPT + "\n" + text}}
            }},
            {"stream", false},
            {"enable_websearch", true}
        };

        if (!KNOWLEDGE_BASE_IDS.empty()) {
            body["knowledge_base_ids"] = json::array({KNOWLEDGE_BASE_IDS});
        }

        auto chat = openai::chat().create(body);
        response_string = extract_message_content(chat);
        if (response_string.empty()) {
            file << "\n[WARN] No textual content found in temporary response. Full payload:\n"
                 << chat.dump(2) << "\n";
            say_error(std::string{"[WARN] Analysis["} + std::to_string(analysis_id) +
                      "] returned no text content; see results file.\n");
        }

        file << "\n\nTemporary response received:\n" << response_string << "\n";
        speak_text("Temporary Analysis[" + std::to_string(analysis_id) + "] completed. Response: " + response_string);
    } catch (const std::exception& e) {
        file << "\n[ERROR] Analysis[" << analysis_id << "] failed: " << e.what() << "\n";
        say_error(std::string{"[ERROR] Analysis["} + std::to_string(analysis_id) + "] failed: " + e.what() + "\n");
    }

    if (!file) {
        say_error("[ERROR] Writing to results file failed for Analysis[" + std::to_string(analysis_id) + "]\n");
    }

    say_info("Temporary Analysis of Recording[" + std::to_string(analysis_id) + "] Finished ------------------->>>\n");
}

// Main loop
int main() {
    if (!load_config("./config.ini")) {
        say_error("Failed to load config.ini\n");
        return 1;
    }

    say_info("Listening for input...\n");

    std::string line;
    std::string collected_text;
    bool collect_text = false;

    while (std::getline(std::cin, line)) {
        std::cout << line << std::endl;

        std::string lower_line = line;
        std::transform(lower_line.begin(), lower_line.end(), lower_line.begin(), ::tolower);

        const bool line_contains_start = contains_substring(lower_line, TRIGGER_START);
        const bool line_contains_stop = contains_substring(lower_line, TRIGGER_STOP);
        const bool line_contains_temp_check = contains_substring(lower_line, TRIGGER_TEMP_CHECK);

        if (line_contains_start) {
            if (collect_text) {
                say_info("Recording has already been started ------------------->>>\n");
            } else {
                say_info("Recording started ------------------->>>\n");
                collected_text.clear();
                collect_text = true;
            }
        }

        if (line_contains_stop) {
            if (!collect_text) {
                say_info("No recording is currently running ------------------->>>\n");
            } else {
                say_info("Recording stopped ------------------->>>\n");
                std::string text_to_analyze = collected_text;
                collected_text.clear();
                collect_text = false;
                if (active_analyses.load() > 0) {
                    say_info("Another analysis is running; this one will start once it finishes ------------------->>>\n");
                }
                std::thread(analyze_text, std::move(text_to_analyze)).detach();
            }
        }

        if (line_contains_temp_check) {
            if (!collect_text) {
                say_info("No recording is currently running ------------------->>>\n");
            } else {
                say_info("Temporary check requested ------------------->>>\n");
                if (active_analyses.load() > 0) {
                    say_info("Another analysis is running; this one will start once it finishes ------------------->>>\n");
                }
                std::string snapshot = collected_text;
                std::thread(temp_analyze_text, std::move(snapshot)).detach();
            }
        }

        if (collect_text && !line_contains_start && !line_contains_stop && !line_contains_temp_check) {
            collected_text += line + "\n";
        }
    }

    return 0;
}
